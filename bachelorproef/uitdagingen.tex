\chapter{Huidige en toekomstige uitdagingen}
\label{ch:uitdagingen}

In dit korte hoofdstuk zullen enkele huidige uitdagingen van \gls{datacompressie} toegelicht worden. Hiermee zal worden aangetoond dat \gls{datacompressie} een enorm groot begrip is dat veel verder gaat dan alleen \gls{afbeeldingscompressie} en \gls{videocompressie}.

\section{DNA compressie}
\label{sec:uitdagingen-dna-compressie}

Het menselijke DNA kan digitaal voorgesteld worden door een lange lijst van 5 verschillende karakters, gekend als basen. Deze digitale voorstelling bestaat uit meer dan 3 miljard van deze basen (\cite{dodanaugent2011}). DNA compressie bestaat er uit deze reeks van basen zo efficiënt mogelijk op te slaan zodanig performante bewerkingen mogelijk zijn met een zo klein mogelijke bestandsgrootte.

Hoewel met de huidige technologie en kennis reeds tal van informatie uit DNA gehaald kan worden, zal verdere uitwerkingen van \gls{dna-compressie} technieken het opslaan van een gigantische hoeveelheid DNA enkel bevorderen.  Dit kan op zijn beurt voor tal van andere doorbraken zorgen. Zo is een AI, die voorzien is van voldoende trainingsdata, mogelijks in staat relaties te vinden tussen DNA en bepaalde aandoeningen dat huidige wetenschappers momenteel niet vinden.

Vakexpert en co-promotor van deze bachelorproef, Tom Paridaens, houd zich op professionele basis bezig met \gls{dna-compressie}.

\subsection{AI en compressie}
\label{sec:uitdagingen-ai}

Zoals besproken in deel \ref{sec:uitdagingen-dna-compressie} is het door verdere databesparing mogelijk meer data te verzamelen en op te slaan. Deze grotere hoeveelheid aan data kan tot een stijging van beschikbare trainingsdata voor AI's zorgen. Dit is ten voordele van de werking van de AI.

Maar ook de bestanden die een AI genereert kunnen verdere \gls{datacompressie} gebruiken. Zo is in Google I/O 2019 besproken dat het model voor natuurlijke stem herkenning verkleind is van ongeveer honderd gigabyte naar nog geen één gigabyte. Dit maakt het mogelijk de modellen te voorzien op toestellen zelf in plaats van in de cloud. Dit wil niet alleen zeggen dat de diensten offline gebruikt kunnen worden maar ook dat de latency aanzienlijk kleiner wordt. 

Meer details over hoe deze verkleining bereikt is, is op het moment van schrijven nog niet beschikbaar maar er zal vermoedelijk gebruik gemaakt zijn van het selectief verwijderen van data aan de hand van een zeer complex \gls{lossy} \gls{compressie-algoritme}. Achterliggend zullen ook tal van \gls{lossless} \glspl{compressie-algoritme} gebruikt zijn om verdere \gls{datacompressie} te bereiken.